{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLC-LLM for Text Embedding in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "model = AutoModel.from_pretrained(\"Snowflake/snowflake-arctic-embed-m\", add_pooling_layer=False)\n",
    "model.eval()\n",
    "\n",
    "query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "queries = [\"what is snowflake?\", \"Where can I get the best tacos?\"]\n",
    "queries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\n",
    "query_tokens = tokenizer(\n",
    "    queries_with_prefix, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")\n",
    "\n",
    "documents = [\"The Data Cloud!\", \"Mexico City of Course!\"]\n",
    "document_tokens = tokenizer(\n",
    "    documents, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[2024-04-30 18:47:09] INFO auto_device.py:79: \u001b[92mFound\u001b[0m device: metal:0\n",
      "[2024-04-30 18:47:09] INFO chat_module.py:379: Using model folder: /Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/dist/snowflake-arctic-embed-m-q0f32-MLC\n",
      "[2024-04-30 18:47:09] INFO chat_module.py:380: Using mlc chat config: /Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/dist/snowflake-arctic-embed-m-q0f32-MLC/mlc-chat-config.json\n"
     ]
    }
   ],
   "source": [
    "from mlc_llm.embeddings.embeddings import MLCEmbeddings\n",
    "\n",
    "mlc_embeddings = MLCEmbeddings(\n",
    "    \"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/dist/snowflake-arctic-embed-m-q0f32-MLC\",\n",
    "    \"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/dist/libs/snowflake-arctic-embed-m-q0f32-metal.so\",\n",
    "    device=\"metal:0\",\n",
    "    # debug_dir=\"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/debug\",\n",
    ")\n",
    "mlc_queries = [\"[CLS] \" + query + \" [SEP]\" for query in queries_with_prefix]\n",
    "mlc_documents = [\"[CLS] \" + document + \" [SEP]\" for document in documents]\n",
    "mlc_tokens = mlc_embeddings._tokenize_queries(mlc_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(len(mlc_tokens[0])):\n",
    "    np.testing.assert_array_equal(mlc_tokens[0][i], query_tokens[\"input_ids\"][i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3311,  0.8075,  0.1499,  ...,  0.6162, -0.0541,  0.2450],\n",
       "         [ 0.3530,  1.0479,  0.0503,  ...,  0.9943, -0.1670,  0.3151],\n",
       "         [ 0.2639,  1.1741, -0.1657,  ...,  1.0260,  0.0698,  0.0333],\n",
       "         ...,\n",
       "         [ 0.1866,  0.8087,  0.0610,  ...,  0.6060, -0.0143,  0.5403],\n",
       "         [ 0.2869,  0.8392,  0.1528,  ...,  0.6220, -0.0822,  0.4867],\n",
       "         [ 0.1527,  0.8309,  0.1612,  ...,  0.5274, -0.0802,  0.5454]],\n",
       "\n",
       "        [[-0.1421, -0.0361,  0.6161,  ...,  0.2524,  0.0108,  0.5810],\n",
       "         [ 0.1238,  0.0119,  0.4495,  ...,  0.2463, -0.1507,  0.9454],\n",
       "         [-0.0113, -0.0466,  0.2424,  ...,  0.5182, -0.1246,  1.1071],\n",
       "         ...,\n",
       "         [-0.1490,  0.3295,  0.2802,  ...,  0.2021, -0.1088,  1.0623],\n",
       "         [-0.2999, -0.1606,  0.3321,  ...,  0.7638, -0.0594,  1.0833],\n",
       "         [ 0.0363,  0.0367,  0.9930,  ...,  0.2296, -0.0787,  0.8901]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings = model(**query_tokens)\n",
    "query_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.3113351e-01  8.0746931e-01  1.4985467e-01 ...  6.1624938e-01\n",
      "   -5.4088268e-02  2.4495500e-01]\n",
      "  [ 3.5296029e-01  1.0478674e+00  5.0326731e-02 ...  9.9431241e-01\n",
      "   -1.6697663e-01  3.1511512e-01]\n",
      "  [ 2.6390970e-01  1.1740766e+00 -1.6567884e-01 ...  1.0260067e+00\n",
      "    6.9798030e-02  3.3341456e-02]\n",
      "  ...\n",
      "  [ 2.4550232e-01  8.0163407e-01  1.1892214e-03 ...  5.2851880e-01\n",
      "    2.9782993e-01  3.5837966e-01]\n",
      "  [ 2.5918993e-01  8.0476511e-01 -4.7530915e-04 ...  5.3083724e-01\n",
      "    2.9855818e-01  3.5750693e-01]\n",
      "  [ 2.5926834e-01  8.0297321e-01 -1.8946523e-03 ...  5.3179914e-01\n",
      "    3.0155331e-01  3.5566795e-01]]\n",
      "\n",
      " [[-1.4205964e-01 -3.6145214e-02  6.1605424e-01 ...  2.5238845e-01\n",
      "    1.0826047e-02  5.8102679e-01]\n",
      "  [ 1.2380671e-01  1.1915212e-02  4.4948661e-01 ...  2.4630266e-01\n",
      "   -1.5065147e-01  9.4537169e-01]\n",
      "  [-1.1339599e-02 -4.6636645e-02  2.4238448e-01 ...  5.1824296e-01\n",
      "   -1.2462155e-01  1.1071267e+00]\n",
      "  ...\n",
      "  [-1.4902829e-01  3.2945088e-01  2.8020367e-01 ...  2.0212011e-01\n",
      "   -1.0883161e-01  1.0623498e+00]\n",
      "  [-2.9992154e-01 -1.6064726e-01  3.3213553e-01 ...  7.6378310e-01\n",
      "   -5.9444539e-02  1.0833368e+00]\n",
      "  [ 3.6348876e-02  3.6688987e-02  9.9298948e-01 ...  2.2962487e-01\n",
      "   -7.8736477e-02  8.9009583e-01]]]\n"
     ]
    }
   ],
   "source": [
    "mlc_query_embeds = mlc_embeddings.embed(mlc_queries).numpy()\n",
    "print(mlc_query_embeds)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    query_embeddings[0].detach().numpy(), mlc_query_embeds, decimal=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is snowflake?\n",
      "0.2747487425804138: The Data Cloud!\n",
      "0.19997990131378174: Mexico City of Course!\n",
      "Query: Where can I get the best tacos?\n",
      "0.29974812269210815: Mexico City of Course!\n",
      "0.2344069629907608: The Data Cloud!\n"
     ]
    }
   ],
   "source": [
    "mlc_query_embeds = mlc_embeddings.embed(mlc_queries).numpy()\n",
    "mlc_document_embeds = mlc_embeddings.embed(mlc_documents).numpy()\n",
    "\n",
    "mlc_query_embeds = mlc_query_embeds[:, 0]\n",
    "mlc_document_embeds = mlc_document_embeds[:, 0]\n",
    "\n",
    "mlc_query_embeds = mlc_query_embeds / np.linalg.norm(mlc_query_embeds, axis=1, keepdims=True)\n",
    "mlc_document_embeds = mlc_document_embeds / np.linalg.norm(\n",
    "    mlc_document_embeds, axis=1, keepdims=True\n",
    ")\n",
    "\n",
    "scores = np.dot(mlc_query_embeds, mlc_document_embeds.T)\n",
    "\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = sorted(zip(documents, query_scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Query: {}\".format(query))\n",
    "    for doc, score in doc_score_pairs:\n",
    "        print(\"{}: {}\".format(score, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is snowflake?\n",
      "tensor(0.2747) The Data Cloud!\n",
      "tensor(0.2000) Mexico City of Course!\n",
      "Query: Where can I get the best tacos?\n",
      "tensor(0.2997) Mexico City of Course!\n",
      "tensor(0.2344) The Data Cloud!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**query_tokens)[0][:, 0]\n",
    "    doument_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "\n",
    "# normalize embeddings\n",
    "query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "doument_embeddings = torch.nn.functional.normalize(doument_embeddings, p=2, dim=1)\n",
    "\n",
    "scores = torch.mm(query_embeddings, doument_embeddings.transpose(0, 1))\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(documents, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    # Output passages & scores\n",
    "    print(\"Query:\", query)\n",
    "    for document, score in doc_score_pairs:\n",
    "        print(score, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcticEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        parsed_texts = [\"[CLS]\" + text + \"[SEP]\" for text in texts]\n",
    "        embed_tokens = mlc_embeddings.embed(parsed_texts).numpy()[:, 0]\n",
    "        embed_tokens = embed_tokens / np.linalg.norm(mlc_query_embeds, axis=1, keepdims=True)\n",
    "        return embed_tokens.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        parsed_text = \"[CLS]\" + text + \"[SEP]\"\n",
    "        embed_tokens = mlc_embeddings.embed([parsed_text]).numpy()[:, 0]\n",
    "        embed_tokens = embed_tokens / np.linalg.norm(mlc_query_embeds, axis=1, keepdims=True)\n",
    "        return embed_tokens.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-04-30 18:47:58] INFO posthog.py:20: Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "[2024-04-30 18:47:58] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n",
      "[2024-04-30 18:47:58] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n",
      "[2024-04-30 18:47:58] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents 0 to 20\n",
      "Adding documents 20 to 40\n",
      "Adding documents 40 to 60\n",
      "Adding documents 60 to 80\n",
      "Adding documents 80 to 100\n",
      "Adding documents 100 to 120\n",
      "Adding documents 120 to 140\n",
      "Adding documents 140 to 160\n",
      "Adding documents 160 to 180\n",
      "Adding documents 180 to 200\n"
     ]
    }
   ],
   "source": [
    "chroma_client = Chroma(\n",
    "    \"mlc_rag\",\n",
    "    GPT4AllEmbeddings(),\n",
    "    \"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/rag\",\n",
    ")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "chunk_size = 20\n",
    "for i in range(0, len(doc_splits), chunk_size):\n",
    "    print(\"Adding documents {} to {}\".format(i, i + chunk_size))\n",
    "    chroma_client.add_documents(doc_splits[i : i + chunk_size])\n",
    "retriever = chroma_client.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
