{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/cfruan/Documents/mlc-llm', '/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python311.zip', '/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python3.11', '/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python3.11/lib-dynload', '', '/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages', '/Users/cfruan/Documents/tvm/python', '/Users/cfruan/Documents/mlc-llm/python']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/cfruan/Documents/tvm/python\")\n",
    "sys.path.append(\"/Users/cfruan/Documents/mlc-llm/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, quantize_embeddings\n",
    "from mlc_llm.embeddings.embeddings import MLCEmbeddings\n",
    "import tvm\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowflake_model_path = \"/Users/cfruan/Documents/models/snowflake-arctic-embed-m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(snowflake_model_path)\n",
    "model = AutoModel.from_pretrained(snowflake_model_path, add_pooling_layer=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "queries = [\"what is snowflake?\", \"Where can I get the best tacos?\"]\n",
    "queries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\n",
    "query_tokens = tokenizer(\n",
    "    queries_with_prefix, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")\n",
    "\n",
    "documents = [\"The Data Cloud!\", \"Mexico City of Course!\"]\n",
    "\n",
    "mlc_queries = [\"[CLS] \" + query + \" [SEP]\" for query in queries_with_prefix]\n",
    "mlc_documents = [\"[CLS] \" + document + \" [SEP]\" for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Embeddings Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 18:53:08] WARNING quantization.py:362: Computing int8 quantization buckets based on 2 embeddings. int8 quantization is more stable with `ranges` calculated from more embeddings or a `calibration_embeddings` that can be used to calculate the buckets.\n"
     ]
    }
   ],
   "source": [
    "# Output is of shape (batch_size, longest_seq_len, hidden_size)\n",
    "# We slice it to (batch_size, hidden_size)\n",
    "query_embeddings_hf = model(**query_tokens)[0][:, 0].detach().numpy()\n",
    "query_int8_hf = quantize_embeddings(query_embeddings_hf, \"int8\")\n",
    "query_binary_hf = quantize_embeddings(query_embeddings_hf, \"binary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLC Embeddings Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-02 18:53:15] INFO chat_module.py:379: Using model folder: /Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC\n",
      "[2024-05-02 18:53:15] INFO chat_module.py:380: Using mlc chat config: /Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC/mlc-chat-config.json\n"
     ]
    }
   ],
   "source": [
    "mlc_embeddings = MLCEmbeddings(\n",
    "    \"/Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC\",\n",
    "    \"/Users/cfruan/Documents/mlc-llm/dist/libs/snowflake-arctic-embed-m-q0f32-metal.so\",\n",
    "    device=\"auto\",\n",
    "    # debug_dir=\"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/debug\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output is of shape (batch_size, longest_seq_len, hidden_size)\n",
    "# We slice it to (batch_size, hidden_size)\n",
    "query_embeddings_mlc = mlc_embeddings.embed(mlc_queries).numpy()[:,0]\n",
    "query_embeddings_mlc_tvm = tvm.nd.array(query_embeddings_mlc, device=mlc_embeddings.device)\n",
    "\n",
    "query_int8_mlc = mlc_embeddings.quantize_embeddings(query_embeddings_mlc_tvm, \"int8\").numpy()\n",
    "query_binary_mlc = mlc_embeddings.quantize_embeddings(query_embeddings_mlc_tvm, \"binary\").numpy()\n",
    "query_binary_mlc = (np.packbits(query_binary_mlc).reshape(query_binary_mlc.shape[0], -1) - 128).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(query_embeddings_mlc, query_embeddings_hf, rtol=0.01)\n",
    "np.testing.assert_allclose(query_int8_mlc, query_int8_mlc)\n",
    "np.testing.assert_allclose(query_binary_mlc, query_binary_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively in a single kernel\n",
    "query_embeddings_mlc = mlc_embeddings.embed_binary(mlc_queries).numpy()\n",
    "query_binary_mlc = (np.packbits(query_embeddings_mlc).reshape(query_embeddings_mlc.shape[0], -1) - 128).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_allclose(query_binary_mlc, query_binary_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
