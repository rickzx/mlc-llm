{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLC-LLM for RAG in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cfruan/miniconda3/envs/mlc-chat-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "snowflake_model_path = \"/Users/cfruan/Documents/models/snowflake-arctic-embed-m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(snowflake_model_path)\n",
    "model = AutoModel.from_pretrained(snowflake_model_path, add_pooling_layer=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare input tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prefix = \"Represent this sentence for searching relevant passages: \"\n",
    "queries = [\"what is snowflake?\", \"Where can I get the best tacos?\"]\n",
    "queries_with_prefix = [\"{}{}\".format(query_prefix, i) for i in queries]\n",
    "query_tokens = tokenizer(\n",
    "    queries_with_prefix, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")\n",
    "\n",
    "documents = [\"The Data Cloud!\", \"Mexico City of Course!\"]\n",
    "document_tokens = tokenizer(\n",
    "    documents, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries_with_prefix[0]:  Represent this sentence for searching relevant passages: what is snowflake?\n",
      "documents[0]:  The Data Cloud!\n",
      "query_tokens[input_ids].shape:  torch.Size([2, 19])\n",
      "document_tokens[input_ids].shape:  torch.Size([2, 7])\n",
      "query_tokens[input_ids][0]:  tensor([  101,  5050,  2023,  6251,  2005,  6575,  7882, 13768,  1024,  2054,\n",
      "         2003,  4586, 10258, 13808,  1029,   102,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "print(\"queries_with_prefix[0]: \", queries_with_prefix[0])\n",
    "print(\"documents[0]: \", documents[0])\n",
    "print(\"query_tokens[input_ids].shape: \", query_tokens[\"input_ids\"].shape)\n",
    "print(\"document_tokens[input_ids].shape: \", document_tokens[\"input_ids\"].shape)\n",
    "\n",
    "# We see the last 3 entries are zero because it is padding due to batching\n",
    "print(\"query_tokens[input_ids][0]: \", query_tokens[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-01 10:11:17] INFO chat_module.py:379: Using model folder: /Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC\n",
      "[2024-05-01 10:11:17] INFO chat_module.py:380: Using mlc chat config: /Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC/mlc-chat-config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlc_queries[0] [CLS] Represent this sentence for searching relevant passages: what is snowflake? [SEP]\n",
      "mlc_documents[0] [CLS] The Data Cloud! [SEP]\n"
     ]
    }
   ],
   "source": [
    "from mlc_llm.embeddings.embeddings import MLCEmbeddings\n",
    "\n",
    "mlc_embeddings = MLCEmbeddings(\n",
    "    \"/Users/cfruan/Documents/mlc-llm/dist/snowflake-arctic-embed-m-q0f32-MLC\",\n",
    "    \"/Users/cfruan/Documents/mlc-llm/dist/libs/snowflake-arctic-embed-m-q0f32-metal.so\",\n",
    "    device=\"metal:0\",\n",
    "    # debug_dir=\"/Users/cfruan/Documents/mlc-llm-repos/mlc-llm-head/debug\",\n",
    ")\n",
    "mlc_queries = [\"[CLS] \" + query + \" [SEP]\" for query in queries_with_prefix]\n",
    "mlc_documents = [\"[CLS] \" + document + \" [SEP]\" for document in documents]\n",
    "mlc_tokens = mlc_embeddings._tokenize_queries(mlc_queries)\n",
    "\n",
    "print(\"mlc_queries[0]\", mlc_queries[0])\n",
    "print(\"mlc_documents[0]\", mlc_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert tokenization of input is the same\n",
    "import numpy as np\n",
    "\n",
    "for i in range(len(mlc_tokens[0])):\n",
    "    np.testing.assert_array_equal(mlc_tokens[0][i], query_tokens[\"input_ids\"][i].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare embeddings output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_embeddings[0].shape:  torch.Size([2, 19, 768])\n",
      "tensor([[[ 0.3311,  0.8075,  0.1499,  ...,  0.6162, -0.0541,  0.2450],\n",
      "         [ 0.3530,  1.0479,  0.0503,  ...,  0.9943, -0.1670,  0.3151],\n",
      "         [ 0.2639,  1.1741, -0.1657,  ...,  1.0260,  0.0698,  0.0333],\n",
      "         ...,\n",
      "         [ 0.1866,  0.8087,  0.0610,  ...,  0.6060, -0.0143,  0.5403],\n",
      "         [ 0.2869,  0.8392,  0.1528,  ...,  0.6220, -0.0822,  0.4867],\n",
      "         [ 0.1527,  0.8309,  0.1612,  ...,  0.5274, -0.0802,  0.5454]],\n",
      "\n",
      "        [[-0.1421, -0.0361,  0.6161,  ...,  0.2524,  0.0108,  0.5810],\n",
      "         [ 0.1238,  0.0119,  0.4495,  ...,  0.2463, -0.1506,  0.9454],\n",
      "         [-0.0113, -0.0466,  0.2424,  ...,  0.5182, -0.1246,  1.1071],\n",
      "         ...,\n",
      "         [-0.1490,  0.3295,  0.2802,  ...,  0.2021, -0.1088,  1.0623],\n",
      "         [-0.2999, -0.1606,  0.3321,  ...,  0.7638, -0.0594,  1.0833],\n",
      "         [ 0.0363,  0.0367,  0.9930,  ...,  0.2296, -0.0787,  0.8901]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Output is of shape (batch_size, longest_seq_len, hidden_size)\n",
    "query_embeddings = model(**query_tokens)\n",
    "print(\"query_embeddings[0].shape: \", query_embeddings[0].shape)\n",
    "print(query_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlc_query_embeds.shape:  (2, 19, 768)\n"
     ]
    }
   ],
   "source": [
    "mlc_query_embeds = mlc_embeddings.embed(mlc_queries).numpy()\n",
    "print(\"mlc_query_embeds.shape: \", mlc_query_embeds.shape)\n",
    "np.testing.assert_array_almost_equal(\n",
    "    query_embeddings[0].detach().numpy(), mlc_query_embeds, decimal=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare score computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is snowflake?\n",
      "0.2747487425804138: The Data Cloud!\n",
      "0.19997990131378174: Mexico City of Course!\n",
      "Query: Where can I get the best tacos?\n",
      "0.29974812269210815: Mexico City of Course!\n",
      "0.2344069629907608: The Data Cloud!\n"
     ]
    }
   ],
   "source": [
    "mlc_query_embeds = mlc_embeddings.embed(mlc_queries).numpy()\n",
    "mlc_document_embeds = mlc_embeddings.embed(mlc_documents).numpy()\n",
    "\n",
    "mlc_query_embeds = mlc_query_embeds[:, 0]\n",
    "mlc_document_embeds = mlc_document_embeds[:, 0]\n",
    "\n",
    "mlc_query_embeds = mlc_query_embeds / np.linalg.norm(mlc_query_embeds, axis=1, keepdims=True)\n",
    "mlc_document_embeds = mlc_document_embeds / np.linalg.norm(\n",
    "    mlc_document_embeds, axis=1, keepdims=True\n",
    ")\n",
    "\n",
    "scores = np.dot(mlc_query_embeds, mlc_document_embeds.T)\n",
    "\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = sorted(zip(documents, query_scores), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Query: {}\".format(query))\n",
    "    for doc, score in doc_score_pairs:\n",
    "        print(\"{}: {}\".format(score, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is snowflake?\n",
      "tensor(0.2747) The Data Cloud!\n",
      "tensor(0.2000) Mexico City of Course!\n",
      "Query: Where can I get the best tacos?\n",
      "tensor(0.2997) Mexico City of Course!\n",
      "tensor(0.2344) The Data Cloud!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    query_embeddings = model(**query_tokens)[0][:, 0]\n",
    "    doument_embeddings = model(**document_tokens)[0][:, 0]\n",
    "\n",
    "\n",
    "# normalize embeddings\n",
    "query_embeddings = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)\n",
    "doument_embeddings = torch.nn.functional.normalize(doument_embeddings, p=2, dim=1)\n",
    "\n",
    "scores = torch.mm(query_embeddings, doument_embeddings.transpose(0, 1))\n",
    "for query, query_scores in zip(queries, scores):\n",
    "    doc_score_pairs = list(zip(documents, query_scores))\n",
    "    doc_score_pairs = sorted(doc_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "    # Output passages & scores\n",
    "    print(\"Query:\", query)\n",
    "    for document, score in doc_score_pairs:\n",
    "        print(score, document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma VectorDB usage\n",
    "\n",
    "Follows https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcticEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        parsed_texts = [\"[CLS]\" + text + \"[SEP]\" for text in texts]\n",
    "        embed_tokens = mlc_embeddings.embed(parsed_texts).numpy()[:, 0]\n",
    "        embed_tokens = embed_tokens / np.linalg.norm(embed_tokens, axis=1, keepdims=True)\n",
    "        return embed_tokens.tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        parsed_text = \"[CLS] Represent this sentence for searching relevant passages: \" + text + \"[SEP]\"\n",
    "        embed_tokens = mlc_embeddings.embed([parsed_text]).numpy()[:, 0]\n",
    "        embed_tokens = embed_tokens / np.linalg.norm(embed_tokens, axis=1, keepdims=True)\n",
    "        return embed_tokens.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-01 10:34:40] INFO segment.py:189: Collection mlc_rag is not created.\n",
      "[2024-05-01 10:34:40] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n",
      "[2024-05-01 10:34:40] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n",
      "[2024-05-01 10:34:40] INFO web_base.py:105: fake_useragent not found, using default user agent.To get a realistic header for requests, `pip install fake_useragent`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents 0 to 20\n",
      "Adding documents 20 to 40\n",
      "Adding documents 40 to 60\n",
      "Adding documents 60 to 80\n",
      "Adding documents 80 to 100\n",
      "Adding documents 100 to 120\n",
      "Adding documents 120 to 140\n",
      "Adding documents 140 to 160\n",
      "Adding documents 160 to 180\n",
      "Adding documents 180 to 200\n"
     ]
    }
   ],
   "source": [
    "chroma_client = Chroma(\n",
    "    \"mlc_rag\",\n",
    "    ArcticEmbeddings(),\n",
    "    \"/Users/cfruan/Documents/mlc-llm/rag\",\n",
    ")\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "chunk_size = 20\n",
    "for i in range(0, len(doc_splits), chunk_size):\n",
    "    print(\"Adding documents {} to {}\".format(i, i + chunk_size))\n",
    "    chroma_client.add_documents(doc_splits[i : i + chunk_size])\n",
    "retriever = chroma_client.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "doc_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
